# -*- coding: utf-8 -*-
"""Regression app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LplxIXOTFQ49qcBDfHFGcupipyHwSRBj
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import io
import base64

# Set page configuration
st.set_page_config(
    page_title="Regression Model Generator",
    page_icon="📊",
    layout="wide"
)

# App title and description
st.title("Regression Model Generator")
st.markdown("""
This app allows you to upload a dataset, build different regression models, and compare their performance.
Upload your CSV or Excel file, select features and target variables, and explore the results!
""")

# Sidebar for model selection and parameters
st.sidebar.header("Model Settings")

# File uploader
uploaded_file = st.sidebar.file_uploader("Upload your dataset (CSV or Excel)", type=['csv', 'xlsx', 'xls'])

# Function to load data
@st.cache_data
def load_data(file):
    if file.name.endswith('csv'):
        return pd.read_csv(file)
    else:
        return pd.read_excel(file)

# Main content
if uploaded_file is not None:
    # Load data
    try:
        df = load_data(uploaded_file)
        st.write("### Dataset Preview")
        st.dataframe(df.head())

        # Basic data info
        col1, col2 = st.columns(2)
        with col1:
            st.write("### Data Shape")
            st.write(f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")
        with col2:
            st.write("### Data Types")
            st.write(df.dtypes)

        # Data preprocessing options
        st.sidebar.header("Data Preprocessing")
        handle_missing = st.sidebar.checkbox("Handle Missing Values", value=True)

        if handle_missing:
            numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
            categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

            missing_numeric = st.sidebar.selectbox(
                "Fill missing numeric values with:",
                options=["Mean", "Median", "Zero", "None"],
                index=0
            )

            missing_categorical = st.sidebar.selectbox(
                "Fill missing categorical values with:",
                options=["Mode", "Unknown", "None"],
                index=0
            )

            # Handle missing values
            if missing_numeric != "None":
                for col in numeric_cols:
                    if df[col].isna().sum() > 0:
                        if missing_numeric == "Mean":
                            df[col].fillna(df[col].mean(), inplace=True)
                        elif missing_numeric == "Median":
                            df[col].fillna(df[col].median(), inplace=True)
                        elif missing_numeric == "Zero":
                            df[col].fillna(0, inplace=True)

            if missing_categorical != "None" and len(categorical_cols) > 0:
                for col in categorical_cols:
                    if df[col].isna().sum() > 0:
                        if missing_categorical == "Mode":
                            df[col].fillna(df[col].mode()[0], inplace=True)
                        elif missing_categorical == "Unknown":
                            df[col].fillna("Unknown", inplace=True)

        # Feature and target selection
        st.write("### Feature and Target Selection")

        # Get numeric columns for potential target selection
        numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()

        target_col = st.selectbox("Select the target variable (the one you want to predict)", numeric_columns)

        # Get remaining columns for feature selection
        remaining_cols = [col for col in df.columns if col != target_col]
        selected_features = st.multiselect("Select features to use in the model",
                                         remaining_cols,
                                         default=remaining_cols)

        if len(selected_features) > 0:
            # Create feature and target datasets
            X = df[selected_features]
            y = df[target_col]

            # Data splitting
            st.sidebar.header("Train-Test Split")
            test_size = st.sidebar.slider("Test Size", 0.1, 0.5, 0.2, 0.05)
            random_state = st.sidebar.number_input("Random State", value=42)

            # Model selection
            st.sidebar.header("Model Selection")
            model_type = st.sidebar.selectbox(
                "Choose a regression model",
                ["Linear Regression", "Ridge Regression", "Lasso Regression",
                 "ElasticNet Regression", "Random Forest Regression", "Gradient Boosting Regression"]
            )

            # Identify categorical features for preprocessing
            categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
            numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

            # Create preprocessing pipelines for numeric and categorical features
            numeric_transformer = Pipeline(steps=[
                ('scaler', StandardScaler())
            ])

            # If categorical features exist, create a transformer for them
            categorical_transformer = Pipeline(steps=[
                ('onehot', OneHotEncoder(handle_unknown='ignore'))
            ]) if categorical_features else None

            # Combine preprocessing steps
            preprocessor_steps = []
            if numeric_features:
                preprocessor_steps.append(('num', numeric_transformer, numeric_features))
            if categorical_features and categorical_transformer:
                preprocessor_steps.append(('cat', categorical_transformer, categorical_features))

            preprocessor = ColumnTransformer(
                transformers=preprocessor_steps
            ) if preprocessor_steps else None

            # Model selection logic
            if model_type == "Linear Regression":
                model = LinearRegression()
            elif model_type == "Ridge Regression":
                alpha = st.sidebar.slider("Alpha value", 0.01, 10.0, 1.0, 0.01)
                model = Ridge(alpha=alpha)
            elif model_type == "Lasso Regression":
                alpha = st.sidebar.slider("Alpha value", 0.01, 10.0, 1.0, 0.01)
                model = Lasso(alpha=alpha)
            elif model_type == "ElasticNet Regression":
                alpha = st.sidebar.slider("Alpha value", 0.01, 10.0, 1.0, 0.01)
                l1_ratio = st.sidebar.slider("L1 ratio", 0.0, 1.0, 0.5, 0.01)
                model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)
            elif model_type == "Random Forest Regression":
                n_estimators = st.sidebar.slider("Number of trees", 50, 500, 100, 10)
                max_depth = st.sidebar.slider("Maximum depth", 1, 20, 10)
                model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)
            elif model_type == "Gradient Boosting Regression":
                n_estimators = st.sidebar.slider("Number of estimators", 50, 500, 100, 10)
                learning_rate = st.sidebar.slider("Learning rate", 0.01, 0.3, 0.1, 0.01)
                model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, random_state=random_state)

            # Create pipeline
            if preprocessor:
                pipeline = Pipeline(steps=[
                    ('preprocessor', preprocessor),
                    ('model', model)
                ])
            else:
                pipeline = Pipeline(steps=[
                    ('model', model)
                ])

            # Split the data
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

            # Train button
            if st.button("Train Model"):
                with st.spinner("Training the model... Please wait."):
                    # Train the model
                    pipeline.fit(X_train, y_train)

                    # Make predictions
                    y_pred_train = pipeline.predict(X_train)
                    y_pred_test = pipeline.predict(X_test)

                    # Calculate metrics
                    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
                    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
                    train_r2 = r2_score(y_train, y_pred_train)
                    test_r2 = r2_score(y_test, y_pred_test)
                    train_mae = mean_absolute_error(y_train, y_pred_train)
                    test_mae = mean_absolute_error(y_test, y_pred_test)

                    # Cross-validation
                    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')

                    # Display metrics
                    st.write("### Model Performance")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.write("**RMSE (Root Mean Squared Error)**")
                        st.write(f"Training RMSE: {train_rmse:.4f}")
                        st.write(f"Testing RMSE: {test_rmse:.4f}")
                    with col2:
                        st.write("**R² Score**")
                        st.write(f"Training R²: {train_r2:.4f}")
                        st.write(f"Testing R²: {test_r2:.4f}")
                    with col3:
                        st.write("**MAE (Mean Absolute Error)**")
                        st.write(f"Training MAE: {train_mae:.4f}")
                        st.write(f"Testing MAE: {test_mae:.4f}")

                    st.write("**Cross-Validation Results (R²)**")
                    st.write(f"Mean CV R²: {np.mean(cv_scores):.4f}")
                    st.write(f"CV R² Scores: {cv_scores}")

                    # Create visualizations
                    st.write("### Visualizations")

                    # Predicted vs Actual plot
                    col1, col2 = st.columns(2)
                    with col1:
                        fig, ax = plt.subplots(figsize=(10, 6))
                        ax.scatter(y_test, y_pred_test, alpha=0.5)
                        ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
                        ax.set_xlabel('Actual')
                        ax.set_ylabel('Predicted')
                        ax.set_title('Actual vs Predicted Values')
                        st.pyplot(fig)

                    # Residual plot
                    with col2:
                        residuals = y_test - y_pred_test
                        fig, ax = plt.subplots(figsize=(10, 6))
                        ax.scatter(y_pred_test, residuals, alpha=0.5)
                        ax.axhline(y=0, color='r', linestyle='--')
                        ax.set_xlabel('Predicted Values')
                        ax.set_ylabel('Residuals')
                        ax.set_title('Residual Plot')
                        st.pyplot(fig)

                    # Feature Importance (if applicable)
                    if model_type in ["Random Forest Regression", "Gradient Boosting Regression"]:
                        feature_names = X.columns.tolist()
                        # Get feature importances (considering possible transformations)
                        if preprocessor and categorical_features:
                            # If we have categorical features that were one-hot encoded
                            # Get the feature names from onehotencoder
                            one_hot_features = []

                            # Get the column transformer
                            if 'cat' in dict(preprocessor.transformers_):
                                cat_transformer = dict(preprocessor.transformers_)['cat']
                                if hasattr(cat_transformer, 'get_feature_names_out'):
                                    one_hot_feature_names = cat_transformer.get_feature_names_out(categorical_features)
                                    one_hot_features = list(one_hot_feature_names)

                            # Combine numeric and transformed categorical feature names
                            feature_names = numeric_features + one_hot_features

                        # Get the model's feature importances
                        try:
                            importances = pipeline.named_steps['model'].feature_importances_
                            # Match the number of feature names with importances
                            if len(feature_names) != len(importances):
                                # If we don't have a match, just use indices
                                feature_names = [f"Feature {i}" for i in range(len(importances))]

                            # Create feature importance dataframe
                            feature_importance_df = pd.DataFrame({
                                'Feature': feature_names[:len(importances)],
                                'Importance': importances
                            }).sort_values(by='Importance', ascending=False)

                            # Plot feature importance
                            fig, ax = plt.subplots(figsize=(10, 8))
                            sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(15), ax=ax)
                            ax.set_title('Feature Importance')
                            st.pyplot(fig)

                            # Display feature importance table
                            st.write("### Feature Importance")
                            st.dataframe(feature_importance_df)
                        except:
                            st.error("Could not compute feature importances")

                    # Display coefficients for linear models
                    if model_type in ["Linear Regression", "Ridge Regression", "Lasso Regression", "ElasticNet Regression"]:
                        try:
                            coef = pipeline.named_steps['model'].coef_
                            intercept = pipeline.named_steps['model'].intercept_

                            # Get feature names considering preprocessing
                            if preprocessor and categorical_features:
                                model_feature_names = []
                                for name, transformer, columns in preprocessor.transformers_:
                                    if name == 'num':
                                        model_feature_names.extend(columns)
                                    elif name == 'cat' and hasattr(transformer, 'get_feature_names_out'):
                                        model_feature_names.extend(transformer.get_feature_names_out(columns))
                            else:
                                model_feature_names = X.columns.tolist()

                            # If number of coefficients doesn't match feature names, use generic names
                            if len(coef) != len(model_feature_names):
                                model_feature_names = [f"Feature {i}" for i in range(len(coef))]

                            # Create coefficients dataframe
                            coef_df = pd.DataFrame({
                                'Feature': model_feature_names[:len(coef)],
                                'Coefficient': coef
                            }).sort_values(by='Coefficient', ascending=False)

                            st.write("### Model Coefficients")
                            st.write(f"Intercept: {intercept:.4f}")

                            # Plot coefficients
                            fig, ax = plt.subplots(figsize=(10, 8))
                            plot_df = coef_df.head(15) if len(coef_df) > 15 else coef_df
                            sns.barplot(x='Coefficient', y='Feature', data=plot_df, ax=ax)
                            ax.set_title('Model Coefficients')
                            st.pyplot(fig)

                            # Display coefficients table
                            st.dataframe(coef_df)
                        except:
                            st.error("Could not compute model coefficients")

                    # Generate equation for linear models
                    if model_type in ["Linear Regression", "Ridge Regression", "Lasso Regression", "ElasticNet Regression"]:
                        try:
                            st.write("### Model Equation")
                            equation = f"y = {intercept:.4f}"

                            for i, (feature, coef_val) in enumerate(zip(model_feature_names[:len(coef)], coef)):
                                if i < 10:  # Only show first 10 terms to avoid overloading
                                    if coef_val >= 0:
                                        equation += f" + {coef_val:.4f} × {feature}"
                                    else:
                                        equation += f" - {abs(coef_val):.4f} × {feature}"

                            if len(coef) > 10:
                                equation += " + ..."

                            st.write(equation)
                        except:
                            st.error("Could not generate model equation")

                    # Prediction tool
                    st.write("### Make Predictions")
                    st.write("Enter values for each feature to make a prediction:")

                    # Create input fields for each feature
                    input_data = {}
                    for feature in selected_features:
                        if feature in numeric_cols:
                            input_data[feature] = st.number_input(f"{feature}",
                                                               value=float(df[feature].mean()),
                                                               format="%.2f")
                        else:
                            input_data[feature] = st.selectbox(f"{feature}",
                                                           options=df[feature].unique())

                    # Make prediction
                    if st.button("Predict"):
                        input_df = pd.DataFrame([input_data])
                        prediction = pipeline.predict(input_df)[0]
                        st.success(f"Predicted {target_col}: {prediction:.4f}")

                    # Export model
                    st.write("### Export Model")
                    if st.button("Download Model Code"):
                        # Generate Python code for the model
                        model_code = f"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score
"""
                        # Import the specific model
                        if model_type == "Linear Regression":
                            model_code += "from sklearn.linear_model import LinearRegression\n"
                        elif model_type == "Ridge Regression":
                            model_code += "from sklearn.linear_model import Ridge\n"
                        elif model_type == "Lasso Regression":
                            model_code += "from sklearn.linear_model import Lasso\n"
                        elif model_type == "ElasticNet Regression":
                            model_code += "from sklearn.linear_model import ElasticNet\n"
                        elif model_type == "Random Forest Regression":
                            model_code += "from sklearn.ensemble import RandomForestRegressor\n"
                        elif model_type == "Gradient Boosting Regression":
                            model_code += "from sklearn.ensemble import GradientBoostingRegressor\n"

                        model_code += f"""
# Load your data (replace with your data loading code)
# df = pd.read_csv('your_data.csv')

# Define features and target
X = df[{selected_features}]
y = df['{target_col}']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size={test_size}, random_state={random_state})

# Identify feature types
categorical_features = {categorical_features}
numeric_features = {numeric_features}

# Create preprocessing steps
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])
"""
                        if categorical_features:
                            model_code += """
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)
"""
                        else:
                            model_code += """
# Combine preprocessing steps (only numeric features)
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features)
    ]
)
"""

                        # Add model definition based on type
                        if model_type == "Linear Regression":
                            model_code += "model = LinearRegression()\n"
                        elif model_type == "Ridge Regression":
                            model_code += f"model = Ridge(alpha={alpha})\n"
                        elif model_type == "Lasso Regression":
                            model_code += f"model = Lasso(alpha={alpha})\n"
                        elif model_type == "ElasticNet Regression":
                            model_code += f"model = ElasticNet(alpha={alpha}, l1_ratio={l1_ratio})\n"
                        elif model_type == "Random Forest Regression":
                            model_code += f"model = RandomForestRegressor(n_estimators={n_estimators}, max_depth={max_depth}, random_state={random_state})\n"
                        elif model_type == "Gradient Boosting Regression":
                            model_code += f"model = GradientBoostingRegressor(n_estimators={n_estimators}, learning_rate={learning_rate}, random_state={random_state})\n"

                        model_code += """
# Create pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])

# Train the model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred_train = pipeline.predict(X_train)
y_pred_test = pipeline.predict(X_test)

# Calculate metrics
train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)

print(f"Training RMSE: {train_rmse:.4f}")
print(f"Testing RMSE: {test_rmse:.4f}")
print(f"Training R²: {train_r2:.4f}")
print(f"Testing R²: {test_r2:.4f}")

# Example of how to make a prediction with new data
# new_data = pd.DataFrame({
#     'feature1': [value1],
#     'feature2': [value2],
#     ...
# })
# prediction = pipeline.predict(new_data)
# print(f"Prediction: {prediction[0]}")
"""

                        # Provide the code for download
                        b64 = base64.b64encode(model_code.encode()).decode()
                        href = f'<a href="data:file/txt;base64,{b64}" download="regression_model.py">Download Python Code</a>'
                        st.markdown(href, unsafe_allow_html=True)

                        # Display the code
                        st.code(model_code, language='python')

        else:
            st.warning("Please select at least one feature to build the model.")

    except Exception as e:
        st.error(f"Error: {e}")
        st.warning("Please check your file format and try again.")
else:
    st.info("Please upload a dataset to get started.")

# Add footer
st.markdown("---")
st.markdown("Developed with ❤️ using Streamlit and scikit-learn")